{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "import itertools\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.keras.models import Model ,load_model\n",
    "from tensorflow.keras.layers import Input, Dense,Concatenate, Reshape, Activation, BatchNormalization,Flatten, Embedding, Dot, Dropout\n",
    "\n",
    "## GPU selection\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I wrote the code in a hacky way to make debugging easier. That is, this hacky code allows you to treat an abstract weight matrix as a class variable like a keras symbolic tensor.    \n",
    "class DoGenVec(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, Info):\n",
    "        super(DoGenVec, self).__init__()\n",
    "        self.NumCl = Info[0]\n",
    "        self.EmbedDim = Info[1]\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'EmbedDim': self.EmbedDim , 'NumCl': self.NumCl })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        np.random.seed(1)\n",
    "        self.GenVec = tf.Variable(np.random.normal(0, 0.1, size=(self.NumCl, self.EmbedDim)).astype(np.float32), trainable=True, name='GenVec')\n",
    "\n",
    "    def call(self, input):\n",
    "        input = K.sum(input) * 0 + 1 # To return only the weight matrix, make the previous input tensor value a scalar 1 and multiply it with the weight matrix. It's just a hacky trick\n",
    "        return (input*self.GenVec)\n",
    "\n",
    "\n",
    "### I wrote the code in a hacky way to make debugging easier. That is, this hacky code allows you to treat an abstract weight matrix as a class variable like a keras symbolic tensor.    \n",
    "class SuppleVec(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, Info):\n",
    "        super(SuppleVec, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.GenVec1 = tf.constant(ExogDTL, name='gen1')\n",
    "        self.GenVec2 = tf.constant(ExogPRM, name='gen2')\n",
    "        self.GenVec3 = tf.constant(np.arange(len(ExpData.columns)) + 1)\n",
    "\n",
    "    def call(self, input):\n",
    "        return (self.GenVec1, self.GenVec2, self.GenVec3 )    \n",
    "    \n",
    "    \n",
    "def soft_rank(x, tau=1.0):\n",
    "    x_exp = tf.expand_dims(x, -1)\n",
    "    x_exp_t = tf.expand_dims(x, -2)\n",
    "    pairwise_diff = x_exp - x_exp_t\n",
    "    pairwise_rank = tf.math.sigmoid(-pairwise_diff / tau)\n",
    "    rank = tf.reduce_sum(pairwise_rank, axis=-1)\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalData = pd.read_csv('../../1.Data/ProcessedData/TotalData.csv')\n",
    "IntToGene = np.load('../../1.Data/ProcessedData/IntToGene_GroupNorm.npy', allow_pickle=True).tolist()\n",
    "IntToPat = np.load('../../1.Data/ProcessedData/IntToPat_GroupNorm.npy', allow_pickle=True).tolist()\n",
    "DistMat = np.load('../../1.Data/ProcessedData/DisimInd_GroupNorm.npy', allow_pickle=True)\n",
    "TTE = np.load('../../1.Data/ProcessedData/TTE_GroupNorm.npy', allow_pickle=True)\n",
    "EVENT = np.load('../../1.Data/ProcessedData/Event_GroupNorm.npy', allow_pickle=True)\n",
    "\n",
    "ExpData = TotalData.drop(columns=['patient_id','tumor_type','time','event']).T\n",
    "ExpData.columns = ExpData.columns +1\n",
    "ExpData = ExpData.set_index(np.arange(len(ExpData))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking vectors used for learning risk ordered embedding vectors\n",
    "NegativeMask = ((TTE[:,None] - TTE[None])<0).astype('int')\n",
    "NegativeNonEvent = NegativeMask * (1-EVENT[None])\n",
    "PositiveMask = ((TTE[:,None] - TTE[None])>0).astype('int')\n",
    "PositiveEvent = PositiveMask * EVENT[None]\n",
    "TrIndEmbeddMask = (NegativeNonEvent + PositiveEvent).astype('float32')\n",
    "\n",
    "# Setting the reference ID (Longest survivor vs shortest death)\n",
    "ReferencePatIDLong = np.argmax(TTE * (EVENT==0).astype('float32'))\n",
    "ReferencePatIDShort = np.where( (TTE == np.min(TTE [EVENT.astype('bool')]) ) & (EVENT==1))[0][0]\n",
    "\n",
    "# Data processing for deep learning model training\n",
    "NormDismInd = ((DistMat - DistMat.min()) / (DistMat.max() - DistMat.min())) \n",
    "NormDismInd = tf.constant(NormDismInd, dtype=tf.float32)\n",
    "TrIndEmbeddMask = tf.constant(TrIndEmbeddMask, dtype=tf.float32)\n",
    "\n",
    "ExogDTL = NormDismInd\n",
    "ExogPRM = TrIndEmbeddMask\n",
    "LOCPEM_lr = ReferencePatIDLong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbedSize = 50\n",
    "NCL_Feat = 5\n",
    "NCL_Ind = 2\n",
    "IndN = len(ExpData.columns)\n",
    "FeatN = len(ExpData)\n",
    "window_size = 3\n",
    "SR_Tau = 0.005\n",
    "\n",
    "XI1 = 0.001 #ξ_1\n",
    "XI2 = 1. #ξ_2\n",
    "\n",
    "\n",
    "InpFeat = Input(shape=(1,))\n",
    "DTL, PRM , IndIdx = SuppleVec([])(InpFeat)\n",
    "BatchInpIdx = tf.cast(InpFeat, dtype=tf.int32)\n",
    "\n",
    "GEM = tf.transpose(DoGenVec([EmbedSize, FeatN + 1])(InpFeat))\n",
    "GEMNorm = tf.linalg.l2_normalize(GEM[:], axis=-1)\n",
    "PEM = tf.transpose(DoGenVec([EmbedSize, IndN + 1 ])(InpFeat))\n",
    "PEMNorm = tf.linalg.l2_normalize(PEM[1:], axis=-1)\n",
    "PCM = DoGenVec([NCL_Ind, EmbedSize ])(InpFeat)\n",
    "GCM = DoGenVec([1, NCL_Feat ])(InpFeat)\n",
    "GCM = tf.nn.softplus(GCM)\n",
    "PCMNorm = tf.linalg.l2_normalize(PCM, axis=-1)\n",
    "\n",
    "GEMBatch = tf.nn.embedding_lookup(GEM[:], tf.cast(InpFeat, dtype=tf.int32))[:,0]\n",
    "GEMBatchNorm = tf.linalg.l2_normalize(GEMBatch, axis=-1) \n",
    "\n",
    "Out = tf.matmul(GEMBatch,PEM[1:], transpose_b=True )\n",
    "Out = Reshape((-1,), name='OutVal')(Out) \n",
    "\n",
    "## Patient clustering\n",
    "# CHLoss_pat \n",
    "ICosCLSim = tf.matmul( PEMNorm, PCMNorm, transpose_b=True)\n",
    "ICosTheta = tf.acos(K.clip(ICosCLSim, -1.+K.epsilon(), 1.0-K.epsilon()))\n",
    "ICosCLDist = ICosTheta/np.pi\n",
    "IMinCLDist = tf.reduce_min(ICosCLDist, axis=-1, keepdims=True)\n",
    "CHL_pat = tf.reduce_mean(tf.maximum(IMinCLDist-XI2, K.epsilon()))\n",
    "\n",
    "# RLoss_pat\n",
    "IREmbeddAssoSim =  tf.matmul(PEMNorm, PEMNorm, transpose_b=True)\n",
    "IREmbeddTheta = tf.acos(K.clip(IREmbeddAssoSim, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "PED = IREmbeddTheta/np.pi\n",
    "RL_pat = tf.reduce_mean(PRM*(PED - DTL )**2)  \n",
    "\n",
    "\n",
    "\n",
    "# Sort Out by risk\n",
    "PEM_lr = tf.nn.embedding_lookup(PEM[1:], tf.cast([[LOCPEM_lr]], dtype=tf.int32))[:,0]\n",
    "PEM_lrNorm = tf.linalg.l2_normalize(PEM_lr, axis=-1)\n",
    "\n",
    "LrCosCLSim = tf.matmul( PEM_lrNorm, PEMNorm, transpose_b=True)\n",
    "LrCosTheta = tf.acos(K.clip(LrCosCLSim, -1.+K.epsilon(), 1.0-K.epsilon()))\n",
    "LrCosCLDist = LrCosTheta/np.pi\n",
    "RiskSortedIDX =  tf.argsort(LrCosCLDist)\n",
    "RiskSortedOut = tf.gather(Out, tf.tile(RiskSortedIDX, (tf.shape(Out)[0], 1)), batch_dims=1)\n",
    "\n",
    "## soft Permutation entropy\n",
    "RiskSortedOutFrame = tf.signal.frame(RiskSortedOut, window_size, 1)\n",
    "Rank = soft_rank(RiskSortedOutFrame, SR_Tau) + 0.5\n",
    "\n",
    "Permutations = list(itertools.permutations(np.arange(1, window_size+1)))\n",
    "Permutations = tf.constant(Permutations, dtype=tf.float32)\n",
    "MVN = tfp.distributions.MultivariateNormalDiag(loc=Permutations[None, :, None], scale_diag=tf.ones(window_size)*0.1)\n",
    "ProbSum = tf.maximum(tf.reduce_sum(MVN.prob(Rank[:, None]), axis=-1), 1e-7)\n",
    "NormProbSum =  ProbSum / tf.reduce_sum(ProbSum, axis=-1, keepdims=True)\n",
    "SoftPeEntropy =-1* tf.reduce_sum(NormProbSum * tf.math.log(NormProbSum), axis=-1)\n",
    "MeanSoftPeEntropy = tf.reduce_mean(SoftPeEntropy)\n",
    "\n",
    "# CHLoss_gene\n",
    "FCLDist = (SoftPeEntropy[:,None] - GCM)**2\n",
    "FMinCLDist = tf.reduce_min(FCLDist, axis=-1, keepdims=True)\n",
    "CHL_gene = tf.reduce_mean(tf.maximum(FMinCLDist-XI1, K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunModel = Model(InpFeat, Out)\n",
    "\n",
    "RunModel.add_loss(CHL_pat)\n",
    "RunModel.add_metric(CHL_pat, 'CHL_pat')\n",
    "\n",
    "#RunModel.add_loss(CHL_gene)\n",
    "#RunModel.add_metric(CHL_gene, 'CHL_gene')\n",
    "\n",
    "RunModel.add_loss(RL_pat)\n",
    "RunModel.add_metric(RL_pat, 'RL_pat')\n",
    "\n",
    "RunModel.add_loss(MeanSoftPeEntropy)\n",
    "RunModel.add_metric(MeanSoftPeEntropy, 'SoftPeEntropy')\n",
    "\n",
    "RunModel.compile(loss='mse', optimizer='adam', metrics={\"OutVal\":'mse' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "8/8 [==============================] - 3s 91ms/step - loss: 2.0367 - mse: 0.1890 - CHL_pat: 1.0000e-07 - RL_pat: 0.0567 - SoftPeEntropy: 1.7910\n",
      "Epoch 2/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 2.0362 - mse: 0.1885 - CHL_pat: 1.0000e-07 - RL_pat: 0.0567 - SoftPeEntropy: 1.7910\n",
      "Epoch 3/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 2.0366 - mse: 0.1889 - CHL_pat: 1.0000e-07 - RL_pat: 0.0567 - SoftPeEntropy: 1.7910\n",
      "Epoch 4/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 2.0363 - mse: 0.1888 - CHL_pat: 1.0000e-07 - RL_pat: 0.0566 - SoftPeEntropy: 1.7910\n",
      "Epoch 5/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 2.0359 - mse: 0.1884 - CHL_pat: 1.0000e-07 - RL_pat: 0.0565 - SoftPeEntropy: 1.7910\n",
      "Epoch 6/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 2.0351 - mse: 0.1878 - CHL_pat: 1.0000e-07 - RL_pat: 0.0563 - SoftPeEntropy: 1.7910\n",
      "Epoch 7/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 2.0349 - mse: 0.1878 - CHL_pat: 1.0000e-07 - RL_pat: 0.0561 - SoftPeEntropy: 1.7909\n",
      "Epoch 8/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 2.0335 - mse: 0.1866 - CHL_pat: 1.0000e-07 - RL_pat: 0.0558 - SoftPeEntropy: 1.7910\n",
      "Epoch 9/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 2.0324 - mse: 0.1860 - CHL_pat: 1.0000e-07 - RL_pat: 0.0553 - SoftPeEntropy: 1.7910\n",
      "Epoch 10/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 2.0297 - mse: 0.1840 - CHL_pat: 1.0000e-07 - RL_pat: 0.0546 - SoftPeEntropy: 1.7910\n",
      "Epoch 11/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 2.0269 - mse: 0.1822 - CHL_pat: 1.0000e-07 - RL_pat: 0.0535 - SoftPeEntropy: 1.7909\n",
      "Epoch 12/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 2.0223 - mse: 0.1790 - CHL_pat: 1.0000e-07 - RL_pat: 0.0520 - SoftPeEntropy: 1.7910\n",
      "Epoch 13/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 2.0171 - mse: 0.1756 - CHL_pat: 1.0000e-07 - RL_pat: 0.0500 - SoftPeEntropy: 1.7910\n",
      "Epoch 14/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 2.0090 - mse: 0.1700 - CHL_pat: 1.0000e-07 - RL_pat: 0.0475 - SoftPeEntropy: 1.7909\n",
      "Epoch 15/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.9987 - mse: 0.1627 - CHL_pat: 1.0000e-07 - RL_pat: 0.0443 - SoftPeEntropy: 1.7909\n",
      "Epoch 16/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.9856 - mse: 0.1531 - CHL_pat: 1.0000e-07 - RL_pat: 0.0408 - SoftPeEntropy: 1.7909\n",
      "Epoch 17/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.9719 - mse: 0.1431 - CHL_pat: 1.0000e-07 - RL_pat: 0.0370 - SoftPeEntropy: 1.7909\n",
      "Epoch 18/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.9558 - mse: 0.1309 - CHL_pat: 1.0000e-07 - RL_pat: 0.0331 - SoftPeEntropy: 1.7909\n",
      "Epoch 19/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.9397 - mse: 0.1187 - CHL_pat: 1.0000e-07 - RL_pat: 0.0293 - SoftPeEntropy: 1.7909\n",
      "Epoch 20/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.9225 - mse: 0.1051 - CHL_pat: 1.0000e-07 - RL_pat: 0.0258 - SoftPeEntropy: 1.7909\n",
      "Epoch 21/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.9057 - mse: 0.0914 - CHL_pat: 1.0000e-07 - RL_pat: 0.0228 - SoftPeEntropy: 1.7909\n",
      "Epoch 22/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8899 - mse: 0.0782 - CHL_pat: 1.0000e-07 - RL_pat: 0.0202 - SoftPeEntropy: 1.7909\n",
      "Epoch 23/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8757 - mse: 0.0663 - CHL_pat: 1.0000e-07 - RL_pat: 0.0181 - SoftPeEntropy: 1.7909\n",
      "Epoch 24/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8626 - mse: 0.0550 - CHL_pat: 1.0000e-07 - RL_pat: 0.0163 - SoftPeEntropy: 1.7909\n",
      "Epoch 25/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8513 - mse: 0.0452 - CHL_pat: 1.0000e-07 - RL_pat: 0.0150 - SoftPeEntropy: 1.7909\n",
      "Epoch 26/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8419 - mse: 0.0369 - CHL_pat: 1.0000e-07 - RL_pat: 0.0139 - SoftPeEntropy: 1.7909\n",
      "Epoch 27/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8344 - mse: 0.0303 - CHL_pat: 1.0000e-07 - RL_pat: 0.0130 - SoftPeEntropy: 1.7909\n",
      "Epoch 28/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8286 - mse: 0.0252 - CHL_pat: 1.0000e-07 - RL_pat: 0.0124 - SoftPeEntropy: 1.7909\n",
      "Epoch 29/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8239 - mse: 0.0211 - CHL_pat: 1.0000e-07 - RL_pat: 0.0118 - SoftPeEntropy: 1.7909\n",
      "Epoch 30/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8203 - mse: 0.0179 - CHL_pat: 1.0000e-07 - RL_pat: 0.0114 - SoftPeEntropy: 1.7909\n",
      "Epoch 31/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8179 - mse: 0.0159 - CHL_pat: 1.0000e-07 - RL_pat: 0.0111 - SoftPeEntropy: 1.7909\n",
      "Epoch 32/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8159 - mse: 0.0141 - CHL_pat: 1.0000e-07 - RL_pat: 0.0108 - SoftPeEntropy: 1.7909\n",
      "Epoch 33/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8146 - mse: 0.0131 - CHL_pat: 1.0000e-07 - RL_pat: 0.0105 - SoftPeEntropy: 1.7908\n",
      "Epoch 34/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8134 - mse: 0.0121 - CHL_pat: 1.0000e-07 - RL_pat: 0.0103 - SoftPeEntropy: 1.7909\n",
      "Epoch 35/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8125 - mse: 0.0114 - CHL_pat: 1.0000e-07 - RL_pat: 0.0101 - SoftPeEntropy: 1.7909\n",
      "Epoch 36/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8118 - mse: 0.0109 - CHL_pat: 1.0000e-07 - RL_pat: 0.0100 - SoftPeEntropy: 1.7909\n",
      "Epoch 37/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8113 - mse: 0.0105 - CHL_pat: 1.0000e-07 - RL_pat: 0.0098 - SoftPeEntropy: 1.7908\n",
      "Epoch 38/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8108 - mse: 0.0102 - CHL_pat: 1.0000e-07 - RL_pat: 0.0097 - SoftPeEntropy: 1.7909\n",
      "Epoch 39/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8102 - mse: 0.0098 - CHL_pat: 1.0000e-07 - RL_pat: 0.0095 - SoftPeEntropy: 1.7908\n",
      "Epoch 40/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8099 - mse: 0.0096 - CHL_pat: 1.0000e-07 - RL_pat: 0.0094 - SoftPeEntropy: 1.7909\n",
      "Epoch 41/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8097 - mse: 0.0095 - CHL_pat: 1.0000e-07 - RL_pat: 0.0093 - SoftPeEntropy: 1.7909\n",
      "Epoch 42/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8094 - mse: 0.0093 - CHL_pat: 1.0000e-07 - RL_pat: 0.0092 - SoftPeEntropy: 1.7909\n",
      "Epoch 43/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8091 - mse: 0.0091 - CHL_pat: 1.0000e-07 - RL_pat: 0.0091 - SoftPeEntropy: 1.7909\n",
      "Epoch 44/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8088 - mse: 0.0089 - CHL_pat: 1.0000e-07 - RL_pat: 0.0090 - SoftPeEntropy: 1.7909\n",
      "Epoch 45/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8088 - mse: 0.0090 - CHL_pat: 1.0000e-07 - RL_pat: 0.0089 - SoftPeEntropy: 1.7908\n",
      "Epoch 46/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8084 - mse: 0.0088 - CHL_pat: 1.0000e-07 - RL_pat: 0.0088 - SoftPeEntropy: 1.7908\n",
      "Epoch 47/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8083 - mse: 0.0088 - CHL_pat: 1.0000e-07 - RL_pat: 0.0087 - SoftPeEntropy: 1.7908\n",
      "Epoch 48/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8082 - mse: 0.0087 - CHL_pat: 1.0000e-07 - RL_pat: 0.0086 - SoftPeEntropy: 1.7909\n",
      "Epoch 49/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8079 - mse: 0.0086 - CHL_pat: 1.0000e-07 - RL_pat: 0.0085 - SoftPeEntropy: 1.7908\n",
      "Epoch 50/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8078 - mse: 0.0085 - CHL_pat: 1.0000e-07 - RL_pat: 0.0084 - SoftPeEntropy: 1.7909\n",
      "Epoch 51/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8077 - mse: 0.0085 - CHL_pat: 1.0000e-07 - RL_pat: 0.0083 - SoftPeEntropy: 1.7909\n",
      "Epoch 52/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 1.8076 - mse: 0.0085 - CHL_pat: 1.0000e-07 - RL_pat: 0.0083 - SoftPeEntropy: 1.7908\n",
      "Epoch 53/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8075 - mse: 0.0084 - CHL_pat: 1.0000e-07 - RL_pat: 0.0082 - SoftPeEntropy: 1.7909\n",
      "Epoch 54/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8072 - mse: 0.0083 - CHL_pat: 1.0000e-07 - RL_pat: 0.0081 - SoftPeEntropy: 1.7908\n",
      "Epoch 55/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8072 - mse: 0.0083 - CHL_pat: 1.0000e-07 - RL_pat: 0.0081 - SoftPeEntropy: 1.7908\n",
      "Epoch 56/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8070 - mse: 0.0082 - CHL_pat: 1.0000e-07 - RL_pat: 0.0080 - SoftPeEntropy: 1.7908\n",
      "Epoch 57/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8070 - mse: 0.0082 - CHL_pat: 1.0000e-07 - RL_pat: 0.0079 - SoftPeEntropy: 1.7908\n",
      "Epoch 58/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8068 - mse: 0.0081 - CHL_pat: 1.0000e-07 - RL_pat: 0.0079 - SoftPeEntropy: 1.7908\n",
      "Epoch 59/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8067 - mse: 0.0081 - CHL_pat: 1.0000e-07 - RL_pat: 0.0078 - SoftPeEntropy: 1.7908\n",
      "Epoch 60/500\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 1.8067 - mse: 0.0081 - CHL_pat: 1.0000e-07 - RL_pat: 0.0077 - SoftPeEntropy: 1.7908\n",
      "Epoch 61/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8065 - mse: 0.0080 - CHL_pat: 1.0000e-07 - RL_pat: 0.0077 - SoftPeEntropy: 1.7908\n",
      "Epoch 62/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8065 - mse: 0.0080 - CHL_pat: 1.0000e-07 - RL_pat: 0.0076 - SoftPeEntropy: 1.7908\n",
      "Epoch 63/500\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 1.8063 - mse: 0.0080 - CHL_pat: 1.0000e-07 - RL_pat: 0.0076 - SoftPeEntropy: 1.7908\n",
      "Epoch 64/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.8063 - mse: 0.0079 - CHL_pat: 1.0000e-07 - RL_pat: 0.0075 - SoftPeEntropy: 1.7908\n",
      "Epoch 65/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8061 - mse: 0.0079 - CHL_pat: 1.0000e-07 - RL_pat: 0.0075 - SoftPeEntropy: 1.7908\n",
      "Epoch 66/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.8061 - mse: 0.0079 - CHL_pat: 1.0000e-07 - RL_pat: 0.0074 - SoftPeEntropy: 1.7907\n",
      "Epoch 67/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8061 - mse: 0.0079 - CHL_pat: 1.0000e-07 - RL_pat: 0.0074 - SoftPeEntropy: 1.7908\n",
      "Epoch 68/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8060 - mse: 0.0078 - CHL_pat: 1.0000e-07 - RL_pat: 0.0073 - SoftPeEntropy: 1.7908\n",
      "Epoch 69/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8059 - mse: 0.0078 - CHL_pat: 1.0000e-07 - RL_pat: 0.0073 - SoftPeEntropy: 1.7908\n",
      "Epoch 70/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8057 - mse: 0.0077 - CHL_pat: 1.0000e-07 - RL_pat: 0.0073 - SoftPeEntropy: 1.7908\n",
      "Epoch 71/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.8057 - mse: 0.0077 - CHL_pat: 1.0000e-07 - RL_pat: 0.0072 - SoftPeEntropy: 1.7908\n",
      "Epoch 72/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8057 - mse: 0.0077 - CHL_pat: 1.0000e-07 - RL_pat: 0.0072 - SoftPeEntropy: 1.7908\n",
      "Epoch 73/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8056 - mse: 0.0078 - CHL_pat: 1.0000e-07 - RL_pat: 0.0071 - SoftPeEntropy: 1.7908\n",
      "Epoch 74/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8056 - mse: 0.0077 - CHL_pat: 1.0000e-07 - RL_pat: 0.0071 - SoftPeEntropy: 1.7908\n",
      "Epoch 75/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8056 - mse: 0.0078 - CHL_pat: 1.0000e-07 - RL_pat: 0.0070 - SoftPeEntropy: 1.7907\n",
      "Epoch 76/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8054 - mse: 0.0077 - CHL_pat: 1.0000e-07 - RL_pat: 0.0070 - SoftPeEntropy: 1.7908\n",
      "Epoch 77/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8055 - mse: 0.0077 - CHL_pat: 1.0000e-07 - RL_pat: 0.0070 - SoftPeEntropy: 1.7908\n",
      "Epoch 78/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8054 - mse: 0.0077 - CHL_pat: 1.0000e-07 - RL_pat: 0.0069 - SoftPeEntropy: 1.7908\n",
      "Epoch 79/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8054 - mse: 0.0077 - CHL_pat: 1.0000e-07 - RL_pat: 0.0069 - SoftPeEntropy: 1.7908\n",
      "Epoch 80/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8053 - mse: 0.0077 - CHL_pat: 1.0000e-07 - RL_pat: 0.0069 - SoftPeEntropy: 1.7908\n",
      "Epoch 81/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8052 - mse: 0.0076 - CHL_pat: 1.0000e-07 - RL_pat: 0.0068 - SoftPeEntropy: 1.7908\n",
      "Epoch 82/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8052 - mse: 0.0076 - CHL_pat: 1.0000e-07 - RL_pat: 0.0068 - SoftPeEntropy: 1.7908\n",
      "Epoch 83/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8051 - mse: 0.0076 - CHL_pat: 1.0000e-07 - RL_pat: 0.0067 - SoftPeEntropy: 1.7907\n",
      "Epoch 84/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8051 - mse: 0.0076 - CHL_pat: 1.0000e-07 - RL_pat: 0.0067 - SoftPeEntropy: 1.7908\n",
      "Epoch 85/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8051 - mse: 0.0076 - CHL_pat: 1.0000e-07 - RL_pat: 0.0067 - SoftPeEntropy: 1.7908\n",
      "Epoch 86/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8050 - mse: 0.0076 - CHL_pat: 1.0000e-07 - RL_pat: 0.0066 - SoftPeEntropy: 1.7908\n",
      "Epoch 87/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8049 - mse: 0.0075 - CHL_pat: 1.0000e-07 - RL_pat: 0.0066 - SoftPeEntropy: 1.7907\n",
      "Epoch 88/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8049 - mse: 0.0076 - CHL_pat: 1.0000e-07 - RL_pat: 0.0066 - SoftPeEntropy: 1.7908\n",
      "Epoch 89/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8048 - mse: 0.0075 - CHL_pat: 1.0000e-07 - RL_pat: 0.0065 - SoftPeEntropy: 1.7908\n",
      "Epoch 90/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8047 - mse: 0.0075 - CHL_pat: 1.0000e-07 - RL_pat: 0.0065 - SoftPeEntropy: 1.7908\n",
      "Epoch 91/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8047 - mse: 0.0075 - CHL_pat: 1.0000e-07 - RL_pat: 0.0065 - SoftPeEntropy: 1.7907\n",
      "Epoch 92/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.8046 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0064 - SoftPeEntropy: 1.7907\n",
      "Epoch 93/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8047 - mse: 0.0075 - CHL_pat: 1.0000e-07 - RL_pat: 0.0064 - SoftPeEntropy: 1.7908\n",
      "Epoch 94/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8045 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0063 - SoftPeEntropy: 1.7907\n",
      "Epoch 95/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8044 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0063 - SoftPeEntropy: 1.7908\n",
      "Epoch 96/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8045 - mse: 0.0075 - CHL_pat: 1.0000e-07 - RL_pat: 0.0063 - SoftPeEntropy: 1.7908\n",
      "Epoch 97/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8044 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0062 - SoftPeEntropy: 1.7907\n",
      "Epoch 98/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8044 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0062 - SoftPeEntropy: 1.7908\n",
      "Epoch 99/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8043 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0062 - SoftPeEntropy: 1.7908\n",
      "Epoch 100/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8043 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0061 - SoftPeEntropy: 1.7907\n",
      "Epoch 101/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8042 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0061 - SoftPeEntropy: 1.7907\n",
      "Epoch 102/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8042 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0061 - SoftPeEntropy: 1.7907\n",
      "Epoch 103/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8041 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0060 - SoftPeEntropy: 1.7907\n",
      "Epoch 104/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8041 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0060 - SoftPeEntropy: 1.7907\n",
      "Epoch 105/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8041 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0060 - SoftPeEntropy: 1.7908\n",
      "Epoch 106/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8041 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0059 - SoftPeEntropy: 1.7908\n",
      "Epoch 107/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8040 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0059 - SoftPeEntropy: 1.7907\n",
      "Epoch 108/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8040 - mse: 0.0074 - CHL_pat: 1.0000e-07 - RL_pat: 0.0059 - SoftPeEntropy: 1.7907\n",
      "Epoch 109/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8039 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0058 - SoftPeEntropy: 1.7907\n",
      "Epoch 110/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8038 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0058 - SoftPeEntropy: 1.7907\n",
      "Epoch 111/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8038 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0057 - SoftPeEntropy: 1.7907\n",
      "Epoch 112/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8037 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0057 - SoftPeEntropy: 1.7907\n",
      "Epoch 113/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8036 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0057 - SoftPeEntropy: 1.7907\n",
      "Epoch 114/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8037 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0056 - SoftPeEntropy: 1.7908\n",
      "Epoch 115/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8035 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0056 - SoftPeEntropy: 1.7907\n",
      "Epoch 116/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8035 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0056 - SoftPeEntropy: 1.7906\n",
      "Epoch 117/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8035 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0055 - SoftPeEntropy: 1.7907\n",
      "Epoch 118/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8035 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0055 - SoftPeEntropy: 1.7907\n",
      "Epoch 119/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8035 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0054 - SoftPeEntropy: 1.7907\n",
      "Epoch 120/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8034 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0054 - SoftPeEntropy: 1.7907\n",
      "Epoch 121/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8033 - mse: 0.0072 - CHL_pat: 1.0000e-07 - RL_pat: 0.0054 - SoftPeEntropy: 1.7907\n",
      "Epoch 122/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8033 - mse: 0.0072 - CHL_pat: 1.0000e-07 - RL_pat: 0.0053 - SoftPeEntropy: 1.7907\n",
      "Epoch 123/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8031 - mse: 0.0072 - CHL_pat: 1.0000e-07 - RL_pat: 0.0053 - SoftPeEntropy: 1.7907\n",
      "Epoch 124/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8032 - mse: 0.0072 - CHL_pat: 1.0000e-07 - RL_pat: 0.0052 - SoftPeEntropy: 1.7907\n",
      "Epoch 125/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8031 - mse: 0.0072 - CHL_pat: 1.0000e-07 - RL_pat: 0.0052 - SoftPeEntropy: 1.7907\n",
      "Epoch 126/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8030 - mse: 0.0072 - CHL_pat: 1.0000e-07 - RL_pat: 0.0052 - SoftPeEntropy: 1.7906\n",
      "Epoch 127/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8031 - mse: 0.0072 - CHL_pat: 1.0000e-07 - RL_pat: 0.0051 - SoftPeEntropy: 1.7907\n",
      "Epoch 128/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8032 - mse: 0.0073 - CHL_pat: 1.0000e-07 - RL_pat: 0.0051 - SoftPeEntropy: 1.7908\n",
      "Epoch 129/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8031 - mse: 0.0072 - CHL_pat: 1.0000e-07 - RL_pat: 0.0050 - SoftPeEntropy: 1.7907\n",
      "Epoch 130/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8029 - mse: 0.0071 - CHL_pat: 1.0000e-07 - RL_pat: 0.0050 - SoftPeEntropy: 1.7907\n",
      "Epoch 131/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8029 - mse: 0.0071 - CHL_pat: 1.0000e-07 - RL_pat: 0.0050 - SoftPeEntropy: 1.7908\n",
      "Epoch 132/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8027 - mse: 0.0071 - CHL_pat: 1.0000e-07 - RL_pat: 0.0049 - SoftPeEntropy: 1.7907\n",
      "Epoch 133/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8028 - mse: 0.0072 - CHL_pat: 1.0000e-07 - RL_pat: 0.0049 - SoftPeEntropy: 1.7907\n",
      "Epoch 134/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.8026 - mse: 0.0071 - CHL_pat: 1.0000e-07 - RL_pat: 0.0048 - SoftPeEntropy: 1.7907\n",
      "Epoch 135/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8026 - mse: 0.0071 - CHL_pat: 1.0000e-07 - RL_pat: 0.0048 - SoftPeEntropy: 1.7907\n",
      "Epoch 136/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8026 - mse: 0.0071 - CHL_pat: 1.0000e-07 - RL_pat: 0.0048 - SoftPeEntropy: 1.7907\n",
      "Epoch 137/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8024 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0047 - SoftPeEntropy: 1.7906\n",
      "Epoch 138/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8026 - mse: 0.0071 - CHL_pat: 1.0000e-07 - RL_pat: 0.0047 - SoftPeEntropy: 1.7908\n",
      "Epoch 139/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8024 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0046 - SoftPeEntropy: 1.7908\n",
      "Epoch 140/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8023 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0046 - SoftPeEntropy: 1.7907\n",
      "Epoch 141/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8023 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0045 - SoftPeEntropy: 1.7908\n",
      "Epoch 142/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8021 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0045 - SoftPeEntropy: 1.7906\n",
      "Epoch 143/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8021 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0045 - SoftPeEntropy: 1.7907\n",
      "Epoch 144/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8022 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0044 - SoftPeEntropy: 1.7907\n",
      "Epoch 145/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8021 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0044 - SoftPeEntropy: 1.7906\n",
      "Epoch 146/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8019 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0043 - SoftPeEntropy: 1.7906\n",
      "Epoch 147/500\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 1.8020 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0043 - SoftPeEntropy: 1.7907\n",
      "Epoch 148/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8020 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0043 - SoftPeEntropy: 1.7907\n",
      "Epoch 149/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8020 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0042 - SoftPeEntropy: 1.7907\n",
      "Epoch 150/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8019 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0042 - SoftPeEntropy: 1.7907\n",
      "Epoch 151/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8019 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0042 - SoftPeEntropy: 1.7907\n",
      "Epoch 152/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8019 - mse: 0.0071 - CHL_pat: 1.0000e-07 - RL_pat: 0.0041 - SoftPeEntropy: 1.7907\n",
      "Epoch 153/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8018 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0041 - SoftPeEntropy: 1.7907\n",
      "Epoch 154/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8017 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0040 - SoftPeEntropy: 1.7907\n",
      "Epoch 155/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8016 - mse: 0.0071 - CHL_pat: 1.0000e-07 - RL_pat: 0.0040 - SoftPeEntropy: 1.7907\n",
      "Epoch 156/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8016 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0040 - SoftPeEntropy: 1.7906\n",
      "Epoch 157/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8018 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0039 - SoftPeEntropy: 1.7908 0s - loss: 1.8017 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0039 - SoftPeEntropy: 1.\n",
      "Epoch 158/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8017 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0039 - SoftPeEntropy: 1.7908\n",
      "Epoch 159/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8017 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0039 - SoftPeEntropy: 1.7908\n",
      "Epoch 160/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.8015 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0038 - SoftPeEntropy: 1.7907\n",
      "Epoch 161/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8014 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0038 - SoftPeEntropy: 1.7907\n",
      "Epoch 162/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 1.8013 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0037 - SoftPeEntropy: 1.7907\n",
      "Epoch 163/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8013 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0037 - SoftPeEntropy: 1.7907\n",
      "Epoch 164/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.8012 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0037 - SoftPeEntropy: 1.7907\n",
      "Epoch 165/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8013 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0036 - SoftPeEntropy: 1.7908\n",
      "Epoch 166/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8010 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0036 - SoftPeEntropy: 1.7905\n",
      "Epoch 167/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8012 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0036 - SoftPeEntropy: 1.7907\n",
      "Epoch 168/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8013 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0035 - SoftPeEntropy: 1.7907\n",
      "Epoch 169/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.8013 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0035 - SoftPeEntropy: 1.7908\n",
      "Epoch 170/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8012 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0035 - SoftPeEntropy: 1.7907\n",
      "Epoch 171/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 1.8012 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0034 - SoftPeEntropy: 1.7907\n",
      "Epoch 172/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.8011 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0034 - SoftPeEntropy: 1.7907\n",
      "Epoch 173/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8011 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0034 - SoftPeEntropy: 1.7908\n",
      "Epoch 174/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8010 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0033 - SoftPeEntropy: 1.7907\n",
      "Epoch 175/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8010 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0033 - SoftPeEntropy: 1.7907\n",
      "Epoch 176/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.8009 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0033 - SoftPeEntropy: 1.7907\n",
      "Epoch 177/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8008 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0032 - SoftPeEntropy: 1.7907\n",
      "Epoch 178/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8010 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0032 - SoftPeEntropy: 1.7909\n",
      "Epoch 179/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.8008 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0032 - SoftPeEntropy: 1.7907\n",
      "Epoch 180/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.8008 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0032 - SoftPeEntropy: 1.7907\n",
      "Epoch 181/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.8005 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0031 - SoftPeEntropy: 1.7906\n",
      "Epoch 182/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8007 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0031 - SoftPeEntropy: 1.7908\n",
      "Epoch 183/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.8008 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0031 - SoftPeEntropy: 1.7906\n",
      "Epoch 184/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8009 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0030 - SoftPeEntropy: 1.7909\n",
      "Epoch 185/500\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 1.8006 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0030 - SoftPeEntropy: 1.7906\n",
      "Epoch 186/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8007 - mse: 0.0070 - CHL_pat: 1.0000e-07 - RL_pat: 0.0030 - SoftPeEntropy: 1.7908\n",
      "Epoch 187/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8005 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0029 - SoftPeEntropy: 1.7906\n",
      "Epoch 188/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.8006 - mse: 0.0068 - CHL_pat: 1.0000e-07 - RL_pat: 0.0029 - SoftPeEntropy: 1.7908\n",
      "Epoch 189/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8006 - mse: 0.0069 - CHL_pat: 1.0000e-07 - RL_pat: 0.0029 - SoftPeEntropy: 1.7907\n",
      "Epoch 190/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.8004 - mse: 0.0068 - CHL_pat: 1.0000e-07 - RL_pat: 0.0029 - SoftPeEntropy: 1.7906\n",
      "Epoch 191/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8003 - mse: 0.0068 - CHL_pat: 1.0000e-07 - RL_pat: 0.0028 - SoftPeEntropy: 1.7906\n",
      "Epoch 192/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8004 - mse: 0.0068 - CHL_pat: 1.0000e-07 - RL_pat: 0.0028 - SoftPeEntropy: 1.7909\n",
      "Epoch 193/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8004 - mse: 0.0068 - CHL_pat: 1.0000e-07 - RL_pat: 0.0028 - SoftPeEntropy: 1.7908\n",
      "Epoch 194/500\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 1.8002 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0028 - SoftPeEntropy: 1.7908\n",
      "Epoch 195/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8002 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0027 - SoftPeEntropy: 1.7907\n",
      "Epoch 196/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8001 - mse: 0.0068 - CHL_pat: 1.0000e-07 - RL_pat: 0.0027 - SoftPeEntropy: 1.7906\n",
      "Epoch 197/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.8003 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0027 - SoftPeEntropy: 1.7908\n",
      "Epoch 198/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8001 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0027 - SoftPeEntropy: 1.7908\n",
      "Epoch 199/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.8001 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0026 - SoftPeEntropy: 1.7908\n",
      "Epoch 200/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.8000 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0026 - SoftPeEntropy: 1.7907\n",
      "Epoch 201/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.8001 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0026 - SoftPeEntropy: 1.7907\n",
      "Epoch 202/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.7998 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0026 - SoftPeEntropy: 1.7906\n",
      "Epoch 203/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.8000 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0026 - SoftPeEntropy: 1.7908\n",
      "Epoch 204/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.8000 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0025 - SoftPeEntropy: 1.7908\n",
      "Epoch 205/500\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 1.7999 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0025 - SoftPeEntropy: 1.7907\n",
      "Epoch 206/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7999 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0025 - SoftPeEntropy: 1.7907\n",
      "Epoch 207/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7999 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0025 - SoftPeEntropy: 1.7908\n",
      "Epoch 208/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7998 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0025 - SoftPeEntropy: 1.7907\n",
      "Epoch 209/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7994 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0024 - SoftPeEntropy: 1.7904\n",
      "Epoch 210/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7997 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0024 - SoftPeEntropy: 1.7908\n",
      "Epoch 211/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7997 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0024 - SoftPeEntropy: 1.7907\n",
      "Epoch 212/500\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 1.8000 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0024 - SoftPeEntropy: 1.7908\n",
      "Epoch 213/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.7996 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0024 - SoftPeEntropy: 1.7907\n",
      "Epoch 214/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7996 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0023 - SoftPeEntropy: 1.7906\n",
      "Epoch 215/500\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 1.7998 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0023 - SoftPeEntropy: 1.7907\n",
      "Epoch 216/500\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 1.7999 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0023 - SoftPeEntropy: 1.7909\n",
      "Epoch 217/500\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 1.7998 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0023 - SoftPeEntropy: 1.7908\n",
      "Epoch 218/500\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 1.7997 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0023 - SoftPeEntropy: 1.7906\n",
      "Epoch 219/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7997 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0022 - SoftPeEntropy: 1.7908\n",
      "Epoch 220/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7995 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0022 - SoftPeEntropy: 1.7906\n",
      "Epoch 221/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.7998 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0022 - SoftPeEntropy: 1.7908\n",
      "Epoch 222/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7996 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0022 - SoftPeEntropy: 1.7907\n",
      "Epoch 223/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7997 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0022 - SoftPeEntropy: 1.7908\n",
      "Epoch 224/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7997 - mse: 0.0067 - CHL_pat: 1.0000e-07 - RL_pat: 0.0021 - SoftPeEntropy: 1.7909\n",
      "Epoch 225/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7996 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0021 - SoftPeEntropy: 1.7909\n",
      "Epoch 226/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7993 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0021 - SoftPeEntropy: 1.7906\n",
      "Epoch 227/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7996 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0021 - SoftPeEntropy: 1.7908\n",
      "Epoch 228/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7992 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0021 - SoftPeEntropy: 1.7907\n",
      "Epoch 229/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7993 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0021 - SoftPeEntropy: 1.7907\n",
      "Epoch 230/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7993 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0020 - SoftPeEntropy: 1.7907\n",
      "Epoch 231/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.7993 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0020 - SoftPeEntropy: 1.7907\n",
      "Epoch 232/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7992 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0020 - SoftPeEntropy: 1.7907\n",
      "Epoch 233/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7993 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0020 - SoftPeEntropy: 1.7908\n",
      "Epoch 234/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7994 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0020 - SoftPeEntropy: 1.7908\n",
      "Epoch 235/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7991 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0020 - SoftPeEntropy: 1.7905\n",
      "Epoch 236/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.7993 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0020 - SoftPeEntropy: 1.7907\n",
      "Epoch 237/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7989 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0020 - SoftPeEntropy: 1.7906\n",
      "Epoch 238/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7992 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0019 - SoftPeEntropy: 1.7908\n",
      "Epoch 239/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7994 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0019 - SoftPeEntropy: 1.7908\n",
      "Epoch 240/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7994 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0019 - SoftPeEntropy: 1.7909\n",
      "Epoch 241/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7992 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0019 - SoftPeEntropy: 1.7908\n",
      "Epoch 242/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7990 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0019 - SoftPeEntropy: 1.7906\n",
      "Epoch 243/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7990 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0019 - SoftPeEntropy: 1.7906\n",
      "Epoch 244/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7991 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0019 - SoftPeEntropy: 1.7907\n",
      "Epoch 245/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7989 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0019 - SoftPeEntropy: 1.7907\n",
      "Epoch 246/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7990 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0018 - SoftPeEntropy: 1.7906\n",
      "Epoch 247/500\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 1.7990 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0018 - SoftPeEntropy: 1.7909\n",
      "Epoch 248/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7989 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0018 - SoftPeEntropy: 1.7906\n",
      "Epoch 249/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7990 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0018 - SoftPeEntropy: 1.7907\n",
      "Epoch 250/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.7989 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0018 - SoftPeEntropy: 1.7906\n",
      "Epoch 251/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7990 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0018 - SoftPeEntropy: 1.7907\n",
      "Epoch 252/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7991 - mse: 0.0066 - CHL_pat: 1.0000e-07 - RL_pat: 0.0018 - SoftPeEntropy: 1.7908\n",
      "Epoch 253/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7989 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0018 - SoftPeEntropy: 1.7906\n",
      "Epoch 254/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.7988 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0017 - SoftPeEntropy: 1.7907\n",
      "Epoch 255/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7990 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0017 - SoftPeEntropy: 1.7907\n",
      "Epoch 256/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7984 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0017 - SoftPeEntropy: 1.7905\n",
      "Epoch 257/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7990 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0017 - SoftPeEntropy: 1.7909\n",
      "Epoch 258/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7988 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0017 - SoftPeEntropy: 1.7906\n",
      "Epoch 259/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7989 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0017 - SoftPeEntropy: 1.7908\n",
      "Epoch 260/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7988 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0017 - SoftPeEntropy: 1.7908\n",
      "Epoch 261/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7990 - mse: 0.0065 - CHL_pat: 1.0000e-07 - RL_pat: 0.0017 - SoftPeEntropy: 1.7908\n",
      "Epoch 262/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7988 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0017 - SoftPeEntropy: 1.7906\n",
      "Epoch 263/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7988 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0017 - SoftPeEntropy: 1.7907\n",
      "Epoch 264/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7987 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7907\n",
      "Epoch 265/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7984 - mse: 0.0063 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7906\n",
      "Epoch 266/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7985 - mse: 0.0063 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7907\n",
      "Epoch 267/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7982 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7904\n",
      "Epoch 268/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7987 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7906\n",
      "Epoch 269/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7988 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7908\n",
      "Epoch 270/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7988 - mse: 0.0064 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7908\n",
      "Epoch 271/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7987 - mse: 0.0063 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7908\n",
      "Epoch 272/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7985 - mse: 0.0063 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7907\n",
      "Epoch 273/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7988 - mse: 0.0063 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7909\n",
      "Epoch 274/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7984 - mse: 0.0063 - CHL_pat: 1.0000e-07 - RL_pat: 0.0016 - SoftPeEntropy: 1.7905\n",
      "Epoch 275/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.7986 - mse: 0.0062 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7909\n",
      "Epoch 276/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7985 - mse: 0.0062 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7908\n",
      "Epoch 277/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.7986 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7908\n",
      "Epoch 278/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.7982 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7906\n",
      "Epoch 279/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.7982 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7907\n",
      "Epoch 280/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7984 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7908\n",
      "Epoch 281/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7983 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7906\n",
      "Epoch 282/500\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 1.7979 - mse: 0.0060 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7905\n",
      "Epoch 283/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.7984 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7908\n",
      "Epoch 284/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7981 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7906\n",
      "Epoch 285/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7982 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7906\n",
      "Epoch 286/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7983 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7907\n",
      "Epoch 287/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7983 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7907\n",
      "Epoch 288/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7981 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0015 - SoftPeEntropy: 1.7905\n",
      "Epoch 289/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7984 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7909\n",
      "Epoch 290/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7983 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7908\n",
      "Epoch 291/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7982 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7906\n",
      "Epoch 292/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7985 - mse: 0.0062 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7909\n",
      "Epoch 293/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7983 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7907\n",
      "Epoch 294/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7983 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7907\n",
      "Epoch 295/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7982 - mse: 0.0061 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7907\n",
      "Epoch 296/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.7982 - mse: 0.0060 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7907\n",
      "Epoch 297/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7982 - mse: 0.0060 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7908\n",
      "Epoch 298/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7980 - mse: 0.0060 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7906\n",
      "Epoch 299/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7982 - mse: 0.0059 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7908\n",
      "Epoch 300/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7982 - mse: 0.0060 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7908\n",
      "Epoch 301/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7979 - mse: 0.0059 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7907\n",
      "Epoch 302/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7980 - mse: 0.0059 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7907\n",
      "Epoch 303/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7980 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7909\n",
      "Epoch 304/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7976 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0014 - SoftPeEntropy: 1.7905\n",
      "Epoch 305/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7979 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7908\n",
      "Epoch 306/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7978 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7908\n",
      "Epoch 307/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7978 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7906\n",
      "Epoch 308/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7981 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7909\n",
      "Epoch 309/500\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.7977 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7905\n",
      "Epoch 310/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7975 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7905\n",
      "Epoch 311/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7979 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7907\n",
      "Epoch 312/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7979 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7908\n",
      "Epoch 313/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7978 - mse: 0.0059 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7907\n",
      "Epoch 314/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7978 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7908\n",
      "Epoch 315/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7978 - mse: 0.0059 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7906\n",
      "Epoch 316/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7978 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7907\n",
      "Epoch 317/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7979 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7908\n",
      "Epoch 318/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7979 - mse: 0.0058 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7908\n",
      "Epoch 319/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7978 - mse: 0.0057 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7907\n",
      "Epoch 320/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7977 - mse: 0.0057 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7907\n",
      "Epoch 321/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7978 - mse: 0.0057 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7907\n",
      "Epoch 322/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7975 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7905\n",
      "Epoch 323/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7978 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7909\n",
      "Epoch 324/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7975 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0013 - SoftPeEntropy: 1.7906\n",
      "Epoch 325/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7976 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7907\n",
      "Epoch 326/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7977 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7907\n",
      "Epoch 327/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7976 - mse: 0.0057 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7906\n",
      "Epoch 328/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7977 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7909\n",
      "Epoch 329/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7973 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7904\n",
      "Epoch 330/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7975 - mse: 0.0057 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7906\n",
      "Epoch 331/500\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 1.7971 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7906\n",
      "Epoch 332/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.7975 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7908\n",
      "Epoch 333/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7975 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7905\n",
      "Epoch 334/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7977 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7908\n",
      "Epoch 335/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.7975 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7907\n",
      "Epoch 336/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7975 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7908\n",
      "Epoch 337/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.7975 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7907\n",
      "Epoch 338/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7970 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7905\n",
      "Epoch 339/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7974 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7908\n",
      "Epoch 340/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7972 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7906\n",
      "Epoch 341/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.7973 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7907\n",
      "Epoch 342/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7975 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7907\n",
      "Epoch 343/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7974 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7907\n",
      "Epoch 344/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.7974 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7905\n",
      "Epoch 345/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7973 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7907\n",
      "Epoch 346/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7973 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7907\n",
      "Epoch 347/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 1.7975 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7907\n",
      "Epoch 348/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7973 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0012 - SoftPeEntropy: 1.7906\n",
      "Epoch 349/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.7972 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 350/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7973 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 351/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7974 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 352/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 1.7975 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7908\n",
      "Epoch 353/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 1.7971 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7906\n",
      "Epoch 354/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7972 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 355/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7973 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 356/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7968 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7903\n",
      "Epoch 357/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7973 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7908\n",
      "Epoch 358/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7972 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7906\n",
      "Epoch 359/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.7973 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7908\n",
      "Epoch 360/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7969 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7904\n",
      "Epoch 361/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7973 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7908\n",
      "Epoch 362/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7973 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 363/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7970 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7902\n",
      "Epoch 364/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7973 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 365/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 1.7972 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7906\n",
      "Epoch 366/500\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 1.7972 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7906\n",
      "Epoch 367/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7972 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 368/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7975 - mse: 0.0056 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7909\n",
      "Epoch 369/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7971 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7906\n",
      "Epoch 370/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 1.7973 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 371/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7972 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 372/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7972 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7905\n",
      "Epoch 373/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7974 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7908\n",
      "Epoch 374/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7970 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7905\n",
      "Epoch 375/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7970 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7905\n",
      "Epoch 376/500\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 1.7970 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7906\n",
      "Epoch 377/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 1.7972 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7906\n",
      "Epoch 378/500\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.7972 - mse: 0.0055 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 379/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7972 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7908\n",
      "Epoch 380/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7972 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7907\n",
      "Epoch 381/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7970 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0011 - SoftPeEntropy: 1.7906\n",
      "Epoch 382/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7972 - mse: 0.0054 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7909\n",
      "Epoch 383/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7969 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7906\n",
      "Epoch 384/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7971 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7908\n",
      "Epoch 385/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7972 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7907\n",
      "Epoch 386/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 1.7969 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7904\n",
      "Epoch 387/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 1.7969 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7906\n",
      "Epoch 388/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7970 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7908\n",
      "Epoch 389/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7968 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7906\n",
      "Epoch 390/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 1.7969 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7906\n",
      "Epoch 391/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7968 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7906 0s - loss: 1.7968 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.79\n",
      "Epoch 392/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7970 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7906\n",
      "Epoch 393/500\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.7968 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7906\n",
      "Epoch 394/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7970 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7907\n",
      "Epoch 395/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7965 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7903\n",
      "Epoch 396/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7968 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7905\n",
      "Epoch 397/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7968 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7907\n",
      "Epoch 398/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7969 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7904\n",
      "Epoch 399/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7967 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7906\n",
      "Epoch 400/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7971 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7908\n",
      "Epoch 401/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7971 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7908\n",
      "Epoch 402/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7971 - mse: 0.0053 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7908\n",
      "Epoch 403/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7968 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7905\n",
      "Epoch 404/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 1.7971 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 0.0010 - SoftPeEntropy: 1.7908\n",
      "Epoch 405/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7964 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.9809e-04 - SoftPeEntropy: 1.7906\n",
      "Epoch 406/500\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 1.7968 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.9681e-04 - SoftPeEntropy: 1.7908\n",
      "Epoch 407/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7967 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.9513e-04 - SoftPeEntropy: 1.7906\n",
      "Epoch 408/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7967 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.9281e-04 - SoftPeEntropy: 1.7905\n",
      "Epoch 409/500\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 1.7968 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 9.9041e-04 - SoftPeEntropy: 1.7907\n",
      "Epoch 410/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7968 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.8736e-04 - SoftPeEntropy: 1.7907\n",
      "Epoch 411/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7968 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.8396e-04 - SoftPeEntropy: 1.7906\n",
      "Epoch 412/500\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 1.7967 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.8125e-04 - SoftPeEntropy: 1.7906\n",
      "Epoch 413/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7968 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.7984e-04 - SoftPeEntropy: 1.7907\n",
      "Epoch 414/500\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 1.7967 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.7825e-04 - SoftPeEntropy: 1.7907\n",
      "Epoch 415/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7967 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.7562e-04 - SoftPeEntropy: 1.7906\n",
      "Epoch 416/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7966 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.7297e-04 - SoftPeEntropy: 1.7904\n",
      "Epoch 417/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7966 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.7143e-04 - SoftPeEntropy: 1.7905\n",
      "Epoch 418/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7967 - mse: 0.0050 - CHL_pat: 1.0000e-07 - RL_pat: 9.7129e-04 - SoftPeEntropy: 1.7907\n",
      "Epoch 419/500\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.7966 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.7065e-04 - SoftPeEntropy: 1.7906\n",
      "Epoch 420/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7968 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.6921e-04 - SoftPeEntropy: 1.7904\n",
      "Epoch 421/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7967 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.6770e-04 - SoftPeEntropy: 1.7907\n",
      "Epoch 422/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7968 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.6679e-04 - SoftPeEntropy: 1.7907\n",
      "Epoch 423/500\n",
      "8/8 [==============================] - 1s 87ms/step - loss: 1.7969 - mse: 0.0052 - CHL_pat: 1.0000e-07 - RL_pat: 9.6441e-04 - SoftPeEntropy: 1.7908\n",
      "Epoch 424/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7968 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.6074e-04 - SoftPeEntropy: 1.7907\n",
      "Epoch 425/500\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.7964 - mse: 0.0050 - CHL_pat: 1.0000e-07 - RL_pat: 9.5675e-04 - SoftPeEntropy: 1.7903\n",
      "Epoch 426/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7966 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.5327e-04 - SoftPeEntropy: 1.7907\n",
      "Epoch 427/500\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.7967 - mse: 0.0050 - CHL_pat: 1.0000e-07 - RL_pat: 9.5149e-04 - SoftPeEntropy: 1.7906\n",
      "Epoch 428/500\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 1.7968 - mse: 0.0051 - CHL_pat: 1.0000e-07 - RL_pat: 9.4976e-04 - SoftPeEntropy: 1.7907\n",
      "Epoch 429/500\n",
      "4/8 [==============>...............] - ETA: 0s - loss: 1.7962 - mse: 0.0050 - CHL_pat: 1.0000e-07 - RL_pat: 9.4831e-04 - SoftPeEntropy: 1.7902"
     ]
    }
   ],
   "source": [
    "RunModel.fit(x=ExpData.index.values , y=ExpData, verbose=1, epochs=500, batch_size=2500, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredSoftPeEntropy = Model(InpFeat, SoftPeEntropy)(ExpData.index.values)\n",
    "PredICosCLDist = Model(InpFeat, ICosCLDist)(ExpData.index.values)\n",
    "PredFCLDist = Model(InpFeat, FCLDist)(ExpData.index.values)\n",
    "PredRank = Model(InpFeat, Rank)(ExpData.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([2814,  128], dtype=int64))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(PredICosCLDist, axis=-1), return_counts=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7849612, 1.7859553, 1.7867029, ..., 1.7917532, 1.7917569,\n",
       "       1.791758 ], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(PredSoftPeEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23527afff40>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sort(PredSoftPeEntropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'do_gen_vec_4/GenVec:0' shape=(50, 17527) dtype=float32, numpy=\n",
       " array([[ 0.16243453,  0.02908837, -0.05304639, ...,  0.0967549 ,\n",
       "          0.16752236,  0.00122109],\n",
       "        [ 0.16946778,  0.17687735, -0.24434154, ...,  0.20351033,\n",
       "          0.01146422, -0.02122133],\n",
       "        [-0.11813452,  0.06965515, -0.30029345, ..., -0.1768973 ,\n",
       "         -0.06483164, -0.21791309],\n",
       "        ...,\n",
       "        [ 0.08028132, -0.04939915,  0.08795321, ...,  0.17130908,\n",
       "         -0.04410062, -0.17014025],\n",
       "        [ 0.07850776,  0.10108462, -0.3984738 , ..., -0.0487695 ,\n",
       "         -0.0996479 ,  0.03963216],\n",
       "        [-0.06413594, -0.16045229, -0.20673166, ..., -0.08684859,\n",
       "         -0.06773606, -0.05707029]], dtype=float32)>,\n",
       " <tf.Variable 'do_gen_vec_5/GenVec:0' shape=(50, 2943) dtype=float32, numpy=\n",
       " array([[ 0.16243453,  0.29192582,  0.237738  , ...,  0.13544063,\n",
       "          0.08720778,  0.09771881],\n",
       "        [-0.00899584, -0.21717721,  0.06196576, ...,  0.2054906 ,\n",
       "          0.1920995 ,  0.21648271],\n",
       "        [ 0.13228154, -0.98649377, -0.7453795 , ...,  0.3860192 ,\n",
       "          0.3624151 ,  0.41656813],\n",
       "        ...,\n",
       "        [-0.09183488,  0.00690888, -0.03002212, ..., -0.23846331,\n",
       "         -0.22001006, -0.25103614],\n",
       "        [-0.01122017, -1.8079962 , -0.96349657, ...,  0.93834794,\n",
       "          0.9812782 ,  1.0725952 ],\n",
       "        [-0.0604606 ,  0.11211322,  0.04337002, ..., -0.25885567,\n",
       "         -0.21136661, -0.26793236]], dtype=float32)>,\n",
       " <tf.Variable 'do_gen_vec_6/GenVec:0' shape=(2, 50) dtype=float32, numpy=\n",
       " array([[ 0.16243453, -0.06117564, -0.05281718, -0.10729686,  0.08654077,\n",
       "         -0.23015387,  0.17448118, -0.07612069,  0.03190391, -0.02493704,\n",
       "          0.14621079, -0.20601407, -0.03224172, -0.03840544,  0.11337695,\n",
       "         -0.10998913, -0.01724282, -0.08778584,  0.00422137,  0.05828152,\n",
       "         -0.11006192,  0.11447237,  0.09015907,  0.05024944,  0.0900856 ,\n",
       "         -0.06837279, -0.01228902, -0.09357695, -0.02678881,  0.05303555,\n",
       "         -0.06916607, -0.03967535, -0.06871727, -0.08452056, -0.06712461,\n",
       "         -0.00126646, -0.11173104,  0.02344157,  0.16598022,  0.07420442,\n",
       "         -0.01918356, -0.08876289, -0.07471583,  0.16924547,  0.00508078,\n",
       "         -0.06369957,  0.01909155,  0.21002552,  0.01201589,  0.06172031],\n",
       "        [ 0.03001703, -0.03522499, -0.11425182, -0.03493427, -0.02088942,\n",
       "          0.05866232,  0.08389834,  0.09311021,  0.02855873,  0.08851412,\n",
       "         -0.0754398 ,  0.12528682,  0.05129298, -0.02980928,  0.04885181,\n",
       "         -0.00755717,  0.11316294,  0.15198168,  0.21855754, -0.13964963,\n",
       "         -0.14441139, -0.05044658,  0.01600371,  0.08761689,  0.03156349,\n",
       "         -0.20222013, -0.0306204 ,  0.08279747,  0.02300947,  0.07620112,\n",
       "         -0.02223281, -0.02007581,  0.01865614,  0.04100516,  0.01982997,\n",
       "          0.01190086, -0.06706623,  0.03775638,  0.01218213,  0.11294839,\n",
       "          0.11989179,  0.01851564, -0.0375285 , -0.06387304,  0.04234944,\n",
       "          0.00773401, -0.03438537,  0.00435969, -0.06200008,  0.0698032 ]],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunModel.weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
